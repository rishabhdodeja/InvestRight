# -*- coding: utf-8 -*-
"""StartUp_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vbuebxp_MWdhvfWi9LmcswJF6rQXz8dE

# Download Crunchbase Datasets

*source: GitHub (notpeter/crunchbase-data)*

*release: 2015-08-27*
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from difflib import SequenceMatcher
import math


"""https://www.kaggle.com/kbrookshier/crunchbase-startup-investments

#Data Transformation

***Required New Variables***


##Not-Age Variables

All except age variables - Total count: 21
"""


  #Company Details:
  
  

"""##Age Variables"""

class Date:
    def __init__(self, d, m, y):
        self.d = d
        self.m = m
        self.y = y
monthDays = [31, 28, 31, 30, 31, 30,
             31, 31, 30, 31, 30, 31]
def countLeapYears(d): 
    years = d.y

    if (d.m <= 2):
        years -= 1
    ans = int(years / 4)
    ans -= int(years / 100)
    ans += int(years / 400)
    return ans
def getDifference(dt1, dt2):
    n1 = dt1.y * 365 + dt1.d
    for i in range(0, dt1.m - 1):
        n1 += monthDays[i]
    n1 += countLeapYears(dt1)
 
    n2 = dt2.y * 365 + dt2.d
    for i in range(0, dt2.m - 1):
        n2 += monthDays[i]
    n2 += countLeapYears(dt2)

    return (n2 - n1)

## roundX_age -  Company's age when it did its round X 


"""#Data Cleaning

**Concat all rows for NotAge Variables**

*data transformation needed hours of processing thus data was split and distributed among team members to process parallely*
"""



"""**Cleaning NaN Values**"""

df.isnull().sum()

#drop companies for which we dont have investments data


#numerical columns

#correlation plot
print(df_num.corr())
sns.heatmap(df_num.corr())

"""# XG-Boost Classification"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot
import joblib

#XGB handles missing values very well, thus using raw data itself


"""# Logistic Regression"""



"""**Training Scores**"""

"""#Data Imputation

To get good results with MLP we attempt to regain lost datapoint due to null values by imputing missing values.

Datawig is a package that does this job us. Based on deep learning techniques it imputes missing values of columns by using correlations & training NNs with other columns

https://github.com/awslabs/datawig
"""

pip install datawig

import datawig
df_imputed = datawig.SimpleImputer.complete(df)

df_imputed.to_csv('/content/drive/MyDrive/StartUp_Project/Stratup_AllVariables_cleaned.csv')

"""# MLP"""

# Binary Classification with Keras Neural Network
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

# load dataset

"""## BaseLine Model with 2 Layers"""

# baseline model
def create_baseline():
	# create model
	# Compile model
	#return model

# evaluate model with dataset
"""## Deep Model with 4 Layers"""

def create_larger():
  # create model
  # Compile model
  #return model

# evaluate model with dataset
#print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))


"""## **Final Model:** We will be using *`XGBoost`* as our final model, as it performs substantially well compared to *`Logistic Regression`* & *`MLP`*"""
